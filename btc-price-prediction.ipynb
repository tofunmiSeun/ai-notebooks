{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download historical daily BTC/USD prices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ['ALHPA_AVANTAGE_API_KEY']\n",
    "url = 'https://www.alphavantage.co/query?function=DIGITAL_CURRENCY_DAILY&symbol=BTC&market=USD&apikey={}'.format(api_key)\n",
    "r = requests.get(url)\n",
    "data = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We got 1000 data points'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_data = data['Time Series (Digital Currency Daily)']\n",
    "closing_prices = []\n",
    "for day in daily_data.keys():\n",
    "    closing_price = round(float(daily_data[day]['4b. close (USD)']), 2)\n",
    "    closing_prices.append(closing_price)\n",
    "\n",
    "\"We got {} data points\".format(len(closing_prices))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into batches of 5 sequential price points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [closing_prices[x:x+time_steps] for x in range(0, len(closing_prices), time_steps)]\n",
    "train_test_split_index = int(0.8 * len(chunks))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160,\n",
       " [23248.86, 23141.57, 23492.09, 23554.85, 23157.07],\n",
       " 40,\n",
       " [22719.71, 23455.52, 23821.61, 23107.39, 22797.16])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = chunks[0:train_test_split_index]\n",
    "test_data = chunks[train_test_split_index:]\n",
    "len(training_data), training_data[0], len(test_data), test_data[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn training and test data into pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([160, 5]), torch.Size([40, 5]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = torch.Tensor(training_data) / 10000\n",
    "test = torch.Tensor(test_data) / 10000\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_output = nn.Linear(1 + time_steps, 1)\n",
    "input_to_hidden = nn.Linear(1 + time_steps, time_steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed forward operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(input, hidden):\n",
    "    combined_input = torch.cat((input, hidden), 1)\n",
    "    output = input_to_output(combined_input)\n",
    "    new_hidden = input_to_hidden(combined_input)\n",
    "    return output, new_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7215]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hidden = torch.zeros(1, time_steps)\n",
    "for i in range(0, time_steps):\n",
    "    input = train[0][i:i+1].unsqueeze(1)\n",
    "    output, hidden = forward(input, hidden)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model in randomized batches in one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "learning_rate = 0.0005\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "input_to_output = nn.Linear(1 + time_steps, 1)\n",
    "input_to_hidden = nn.Linear(1 + time_steps, time_steps)\n",
    "\n",
    "def run_epoch():\n",
    "    randomized_training_indices = torch.randperm(train.shape[0])\n",
    "    losses = []\n",
    "    # for each training batch\n",
    "    for new_batch_start in range(0, len(randomized_training_indices), batch_size):\n",
    "        batch_indices = [randomized_training_indices[x].item() for x in range(new_batch_start, new_batch_start+batch_size)]\n",
    "        \n",
    "        predictions = torch.zeros(len(batch_indices))\n",
    "        outputs = torch.zeros(len(batch_indices))\n",
    "        index = 0\n",
    "\n",
    "        input_to_hidden.zero_grad()\n",
    "        input_to_output.zero_grad()\n",
    "    \n",
    "        # for training data in batch\n",
    "        for batch_index in batch_indices:\n",
    "\n",
    "            hidden = torch.zeros(1, time_steps)\n",
    "            single_train_data = train[batch_index]\n",
    "            prediction = 0\n",
    "            # for each time step\n",
    "            for step in range(0, len(single_train_data - 1)):\n",
    "                input = single_train_data[step:step+1].unsqueeze(1)\n",
    "                prediction, hidden = forward(input, hidden)          \n",
    "            \n",
    "            actual = single_train_data[-1]\n",
    "            predictions[index] = prediction\n",
    "            outputs[index] = actual\n",
    "            index += 1\n",
    "        \n",
    "        # calculate error\n",
    "        loss = mse(predictions, outputs)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # back propagation to adjust weights\n",
    "        loss.backward()\n",
    "        for p in input_to_hidden.parameters():\n",
    "            p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "        input_to_hidden.grad = None\n",
    "\n",
    "        for p in input_to_output.parameters():\n",
    "            p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "        input_to_output.grad = None\n",
    "    \n",
    "    print(\"avg epoch loss: {}\".format(sum(losses)/ len(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg epoch loss: 4.754158481955528\n",
      "avg epoch loss: 1.7350148484110832\n",
      "avg epoch loss: 0.5397066846489906\n",
      "avg epoch loss: 0.14634726475924253\n",
      "avg epoch loss: 0.03944894589949399\n",
      "avg epoch loss: 0.014219910284737125\n",
      "avg epoch loss: 0.008661530824610963\n",
      "avg epoch loss: 0.007340395750361495\n",
      "avg epoch loss: 0.007011319074081257\n",
      "avg epoch loss: 0.006899345695273951\n",
      "avg epoch loss: 0.006823368443292566\n",
      "avg epoch loss: 0.006763444063835777\n",
      "avg epoch loss: 0.006699280478642322\n",
      "avg epoch loss: 0.006637948579736985\n",
      "avg epoch loss: 0.006563397793797776\n",
      "avg epoch loss: 0.006511490239063278\n",
      "avg epoch loss: 0.0064420787239214405\n",
      "avg epoch loss: 0.006379861995810643\n",
      "avg epoch loss: 0.006335714380838908\n",
      "avg epoch loss: 0.006272993370657787\n",
      "avg epoch loss: 0.006227822159416974\n",
      "avg epoch loss: 0.0061538691516034305\n",
      "avg epoch loss: 0.006098491765442304\n",
      "avg epoch loss: 0.006035747574060224\n",
      "avg epoch loss: 0.005993629529257305\n",
      "avg epoch loss: 0.0059376349818194285\n",
      "avg epoch loss: 0.005876464259927161\n",
      "avg epoch loss: 0.005832575203385204\n",
      "avg epoch loss: 0.005775210564024746\n",
      "avg epoch loss: 0.005759335414040834\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 30):\n",
    "    run_epoch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.2720, 2.3456, 2.3822, 2.3107, 2.2797])\n",
      "prediction: tensor([[22997.9258]], grad_fn=<MulBackward0>), actual: 22797.16015625, percentage error=0.881\n",
      "tensor([2.1336, 1.9426, 1.9273, 1.9175, 1.8809])\n",
      "prediction: tensor([[19178.1816]], grad_fn=<MulBackward0>), actual: 18808.689453125, percentage error=1.964\n",
      "tensor([1.8037, 1.8255, 1.8541, 1.8324, 1.9167])\n",
      "prediction: tensor([[18345.8672]], grad_fn=<MulBackward0>), actual: 19166.900390625, percentage error=-4.284\n",
      "tensor([1.9359, 1.9148, 1.8651, 1.9422, 1.9204])\n",
      "prediction: tensor([[19207.9961]], grad_fn=<MulBackward0>), actual: 19204.08984375, percentage error=0.02\n",
      "tensor([1.8765, 1.9696, 1.8185, 1.7720, 1.7140])\n",
      "prediction: tensor([[17946.1797]], grad_fn=<MulBackward0>), actual: 17139.51953125, percentage error=4.706\n",
      "tensor([1.7149, 1.8719, 1.9160, 1.8368, 1.8414])\n",
      "prediction: tensor([[18501.1348]], grad_fn=<MulBackward0>), actual: 18414.4296875, percentage error=0.471\n",
      "tensor([1.8704, 1.8656, 1.7803, 1.7776, 1.7659])\n",
      "prediction: tensor([[17859.7363]], grad_fn=<MulBackward0>), actual: 17659.380859375, percentage error=1.135\n",
      "tensor([1.6714, 1.5957, 1.6070, 1.6321, 1.6292])\n",
      "prediction: tensor([[16352.5811]], grad_fn=<MulBackward0>), actual: 16291.8603515625, percentage error=0.373\n",
      "tensor([1.5684, 1.5297, 1.5328, 1.5475, 1.4818])\n",
      "prediction: tensor([[15576.0059]], grad_fn=<MulBackward0>), actual: 14818.2998046875, percentage error=5.113\n",
      "tensor([1.5580, 1.5590, 1.4144, 1.4024, 1.3549])\n",
      "prediction: tensor([[14357.8232]], grad_fn=<MulBackward0>), actual: 13549.369140625, percentage error=5.967\n",
      "tensor([1.3762, 1.3791, 1.3560, 1.3456, 1.3266])\n",
      "prediction: tensor([[13731.8564]], grad_fn=<MulBackward0>), actual: 13266.400390625, percentage error=3.509\n",
      "tensor([1.3636, 1.3052, 1.3029, 1.3112, 1.2923])\n",
      "prediction: tensor([[13352.5029]], grad_fn=<MulBackward0>), actual: 12923.0703125, percentage error=3.323\n",
      "tensor([1.2969, 1.2781, 1.1910, 1.1751, 1.1503])\n",
      "prediction: tensor([[12172.0664]], grad_fn=<MulBackward0>), actual: 11503.1396484375, percentage error=5.815\n",
      "tensor([1.1360, 1.1319, 1.1505, 1.1418, 1.1421])\n",
      "prediction: tensor([[11764.6562]], grad_fn=<MulBackward0>), actual: 11420.5595703125, percentage error=3.013\n",
      "tensor([1.1528, 1.1369, 1.1293, 1.1051, 1.0926])\n",
      "prediction: tensor([[11472.3662]], grad_fn=<MulBackward0>), actual: 10925.5703125, percentage error=5.005\n",
      "tensor([1.0666, 1.0600, 1.0792, 1.0667, 1.0542])\n",
      "prediction: tensor([[11060.1816]], grad_fn=<MulBackward0>), actual: 10542.060546875, percentage error=4.915\n",
      "tensor([1.0570, 1.0619, 1.0777, 1.0840, 1.0696])\n",
      "prediction: tensor([[11180.7432]], grad_fn=<MulBackward0>), actual: 10696.1201171875, percentage error=4.531\n",
      "tensor([1.0774, 1.0729, 1.0687, 1.0736, 1.0241])\n",
      "prediction: tensor([[11099.8105]], grad_fn=<MulBackward0>), actual: 10241.4599609375, percentage error=8.381\n",
      "tensor([1.0530, 1.0417, 1.0920, 1.1081, 1.0933])\n",
      "prediction: tensor([[11363.8438]], grad_fn=<MulBackward0>), actual: 10933.3896484375, percentage error=3.937\n",
      "tensor([1.0940, 1.0954, 1.0785, 1.0672, 1.0333])\n",
      "prediction: tensor([[11085.8311]], grad_fn=<MulBackward0>), actual: 10332.830078125, percentage error=7.287\n",
      "tensor([1.0441, 1.0388, 1.0337, 1.0219, 1.0127])\n",
      "prediction: tensor([[10649.7061]], grad_fn=<MulBackward0>), actual: 10126.650390625, percentage error=5.165\n",
      "tensor([1.0373, 1.0256, 1.0167, 1.0446, 1.0141])\n",
      "prediction: tensor([[10772.8516]], grad_fn=<MulBackward0>), actual: 10140.849609375, percentage error=6.232\n",
      "tensor([1.1389, 1.1922, 1.1650, 1.1711, 1.1466])\n",
      "prediction: tensor([[12032.2266]], grad_fn=<MulBackward0>), actual: 11465.8408203125, percentage error=4.94\n",
      "tensor([1.1527, 1.1330, 1.1461, 1.1318, 1.1748])\n",
      "prediction: tensor([[11689.0361]], grad_fn=<MulBackward0>), actual: 11748.2001953125, percentage error=-0.504\n",
      "tensor([1.1648, 1.1663, 1.1531, 1.1854, 1.1755])\n",
      "prediction: tensor([[12100.6514]], grad_fn=<MulBackward0>), actual: 11754.58984375, percentage error=2.944\n",
      "tensor([1.1945, 1.2281, 1.1911, 1.1852, 1.1761])\n",
      "prediction: tensor([[12204.1475]], grad_fn=<MulBackward0>), actual: 11760.5400390625, percentage error=3.772\n",
      "tensor([1.1780, 1.1564, 1.1392, 1.1893, 1.1682])\n",
      "prediction: tensor([[12100.2080]], grad_fn=<MulBackward0>), actual: 11681.6796875, percentage error=3.583\n",
      "tensor([1.1761, 1.1594, 1.1762, 1.1745, 1.1192])\n",
      "prediction: tensor([[12061.0107]], grad_fn=<MulBackward0>), actual: 11191.9697265625, percentage error=7.765\n",
      "tensor([1.1220, 1.1071, 1.1801, 1.1335, 1.1100])\n",
      "prediction: tensor([[11741.8574]], grad_fn=<MulBackward0>), actual: 11099.6103515625, percentage error=5.786\n",
      "tensor([1.1101, 1.0906, 1.1030, 0.9932, 0.9700])\n",
      "prediction: tensor([[10602.7031]], grad_fn=<MulBackward0>), actual: 9700.419921875, percentage error=9.301\n",
      "tensor([0.9538, 0.9603, 0.9518, 0.9390, 0.9161])\n",
      "prediction: tensor([[9865.7002]], grad_fn=<MulBackward0>), actual: 9160.7802734375, percentage error=7.695\n",
      "tensor([0.9209, 0.9170, 0.9154, 0.9134, 0.9198])\n",
      "prediction: tensor([[9592.7686]], grad_fn=<MulBackward0>), actual: 9197.599609375, percentage error=4.296\n",
      "tensor([0.9256, 0.9243, 0.9303, 0.9234, 0.9288])\n",
      "prediction: tensor([[9694.2051]], grad_fn=<MulBackward0>), actual: 9288.33984375, percentage error=4.37\n",
      "tensor([0.9232, 0.9436, 0.9257, 0.9344, 0.9069])\n",
      "prediction: tensor([[9775.2129]], grad_fn=<MulBackward0>), actual: 9069.41015625, percentage error=7.782\n",
      "tensor([0.9135, 0.9058, 0.9087, 0.9232, 0.9139])\n",
      "prediction: tensor([[9643.4717]], grad_fn=<MulBackward0>), actual: 9138.5498046875, percentage error=5.525\n",
      "tensor([0.9193, 0.9116, 0.9012, 0.9162, 0.9249])\n",
      "prediction: tensor([[9585.0391]], grad_fn=<MulBackward0>), actual: 9249.490234375, percentage error=3.628\n",
      "tensor([0.9296, 0.9625, 0.9686, 0.9295, 0.9359])\n",
      "prediction: tensor([[9825.8223]], grad_fn=<MulBackward0>), actual: 9358.9501953125, percentage error=4.989\n",
      "tensor([0.9310, 0.9386, 0.9465, 0.9526, 0.9426])\n",
      "prediction: tensor([[9937.7393]], grad_fn=<MulBackward0>), actual: 9426.01953125, percentage error=5.429\n",
      "tensor([0.9342, 0.9473, 0.9465, 0.9280, 0.9885])\n",
      "prediction: tensor([[9769.3135]], grad_fn=<MulBackward0>), actual: 9885.0, percentage error=-1.17\n",
      "tensor([0.9772, 0.9782, 0.9747, 0.9666, 0.9621])\n",
      "prediction: tensor([[10113.8809]], grad_fn=<MulBackward0>), actual: 9621.16015625, percentage error=5.121\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, test.shape[0]):\n",
    "    single_test_data = test[i]\n",
    "    print(single_test_data)\n",
    "    hidden = torch.zeros(1, time_steps)\n",
    "    prediction = 0\n",
    "    # for each time step\n",
    "    for step in range(0, len(single_test_data) - 1):\n",
    "        input = single_test_data[step:step+1].unsqueeze(1)\n",
    "        prediction, hidden = forward(input, hidden)          \n",
    "    \n",
    "    actual = single_test_data[-1]\n",
    "    percentage_error = round(((prediction.item() - actual.item()) * 100) / actual.item(), 3)\n",
    "    print(\"prediction: {}, actual: {}, percentage error={}\".format(prediction  * 10000, actual * 10000, percentage_error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12b84d27cf94b059d9386ef0e125d5acfb36035007af7d95356a9eda8a09a0a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
